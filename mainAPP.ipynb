{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhix82x7AVkEhRG46u+zht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tchotaneu/Computer_vision/blob/main/mainAPP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "charger le dossier depuis le drive google "
      ],
      "metadata": {
        "id": "8BUKjFasBpmn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8BxQuA3BriX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## initialisation "
      ],
      "metadata": {
        "id": "YUmf60W6_b3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYEGSyxhcvoO"
      },
      "outputs": [],
      "source": [
        "########################## Amine ######################### @author: Claude PAQUIE\n",
        "import statistics\n",
        "import time\n",
        "import networkx as nx \n",
        "from random import Random\n",
        "import numpy as np\n",
        "from amine.amine import models\n",
        "from amine.amine.datasets import Datasets\n",
        "from amine.amine.module_detection import ModuleDetection\n",
        "from amine.amine.scores import Scores\n",
        "\n",
        "######################### Mane ########################  @author: sezin\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import MANE.without_attention.Node_Classification.generate_pairs as  generate_pairs\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from collections import OrderedDict\n",
        "\n",
        "######################### deplacement des fichiers et dossiers ###############\n",
        "import shutil\n",
        "\n",
        "######################### similarité cosinus  ########################\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## definition des paramatres  de d' AMINE et Mane "
      ],
      "metadata": {
        "id": "fFiBkn46_ocX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################### definition des parametres pour amine #################\n",
        "class argumentAmine:    \n",
        "     number_of_runs=3  #help=\"specifies the number of runs (default=1000)               type = int \n",
        "     nb_modules=1 #specifies the number of modules to generate (default=1)\",    type = int \n",
        "     module_size=10 #specifies the size of each modules (default=10)                  type = int \n",
        "     target_module_size=None#help=\"specifies the expected size of each modules (default=None, i.e. no expected size)\" type = int \n",
        "     network_size=1000 #  help=\"specifies the number of vertices in graph (default=1000)\", type=int,\n",
        "     removed_edges=0.0 # \"proportion des arretes a supprime sur le graphe de donnees la valeur par defaut est  0.0\n",
        "     verbose=False #  help=\"displays results on screen\",\n",
        "     outfile=None #help=\"name of the output file (default=no output)\",\n",
        "     graph_generation=[\"guyondata\", \"guyon\", \"batra\", \"gencat\"]\n",
        "     param_k=1#  \"\"K parameter used by Batra method to specify the average shortest path length\n",
        "     param_mean_bg= 0 , #help=\"mean of the background used by Batra method (default=0)\",\n",
        "     param_mean_fg = 1   # help=\"mean of the foreground used by Batra method (default=1)\", \n",
        "     param_std_bg=0 #=\"mean of the background used by Batra method (default=0)\",\n",
        "     param_std_fg=1 #mean of the foreground used by Batra method (default=1)\"\n",
        "\n",
        "#################### definition des parametres Mane #################\n",
        "\n",
        "class  get_parser():\n",
        "    read_pair=False\n",
        "    input_graphs=\"MANE/data/networks/\"\n",
        "    input_pairs=\"MANE/data/pairs/\"\n",
        "    output=\"MANE/output/emb/\"\n",
        "    output_pairs=\"MANE/output/pairs/\"\n",
        "    dataset=\"Artificiel_Data/\"\n",
        "    nviews=2\n",
        "    dimensions=64\n",
        "    alpha=1.0\n",
        "    beta= 1.0\n",
        "    walk_length=10\n",
        "    num_walks=5\n",
        "    window_size=3\n",
        "    p= 1.0\n",
        "    q=1.0\n",
        "    weighted=False\n",
        "    unweighted=True\n",
        "    directed=False\n",
        "    undirected=True\n",
        "    learning_rate=0.001\n",
        "    negative_sampling=10.0\n",
        "    batch_size=256\n",
        "    epochs=1000# default 10\n",
        "    cuda=True\n"
      ],
      "metadata": {
        "id": "-TsmlSRNdLiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## definition des fonction pour construire la deuxieme vue du dataset "
      ],
      "metadata": {
        "id": "WKlnwWy8_8lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definition de la fonction pour construire un dataset artificielle a partir d'un autre \n",
        "def graph_artificiel1(G: nx.Graph, p_value:float =0.05) : \n",
        "    list_nodes=[] #liste des noeuds du graphe donnée en entree \n",
        "    nodes_pvalue=[] # liste des noeuds ayant une p_value inferieur à 0.05 en considerant leur poids \n",
        "    nodes_ww=[] # liste des noeuds sans leurs poids de p_valeur \n",
        "    edges_Gprime=[]  # liste des arretes du graphe G_prime \n",
        "    G_prime=nx.Graph() # initialisation du graphe G_prime \n",
        "    #print(list(G.nodes.data()))\n",
        "    #construction de la liste des arretes du graphe G_prime \n",
        "    list_nodes=list(G.nodes.data()) # recuperation des l'ensemble des noeuds du graphes donnees en entrees \n",
        "        # selection des noeuds dont le poids est inferieur a la p_value\n",
        "    for i in range (len(list_nodes)):\n",
        "          if (list_nodes[i][1]['weight'])<p_value:\n",
        "            nodes_pvalue.append(list_nodes[i])\n",
        "            nodes_ww.append(list_nodes[i][0])\n",
        "    G_prime.add_nodes_from(nodes_pvalue) #affectation des noeuds au graph G_prime \n",
        "        #construction de la liste des arretes du graphe G_prime \n",
        "    a=0 \n",
        "    #print(nodes_ww)\n",
        "    for i in range(len(nodes_ww)-1):\n",
        "        a=i+1\n",
        "        while a<len(nodes_ww):\n",
        "            edges_Gprime.append((nodes_ww[i],nodes_ww[a],{'weight': None}))\n",
        "            a=a+1\n",
        "    #print(edges_Gprime)\n",
        "    G_prime.add_edges_from(edges_Gprime)\n",
        "    return  Datasets.init_graph(G_prime, edge_weight=None, default_groups=\"truehit\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# definition de la fonction pour construire un dataset artificielle a partir d'un autre \n",
        "def graph_artificiel2(G: nx.Graph, p_value:float =0.05) : \n",
        "    list_nodes=[] #liste des noeuds du graphe donnée en entree \n",
        "    nodes_pvalue=[] # liste des noeuds ayant une p_value inferieur à 0.05 en considerant leur poids \n",
        "    nodes_pvalueGrand=[] # liste des noeuds ayant une p_value inferieur à 0.05 en considerant leur poids \n",
        "    nodes_ww=[] # liste des noeuds sans leurs poids de p_valeur \n",
        "    edges_Gprime=[]  # liste des arretes du graphe G_prime \n",
        "    G_prime=nx.Graph() # initialisation du graphe G_prime \n",
        "    #print(list(G.nodes.data()))\n",
        "    #construction de la liste des arretes du graphe G_prime \n",
        "    list_nodes=list(G.nodes.data()) # recuperation des l'ensemble des noeuds du graphes donnees en entrees \n",
        "        # selection des noeuds dont le poids est inferieur a la p_value\n",
        "    for i in range (len(list_nodes)):\n",
        "          if (list_nodes[i][1]['weight'])<p_value:\n",
        "            nodes_pvalue.append(list_nodes[i])\n",
        "            nodes_ww.append(list_nodes[i][0])\n",
        "          else:\n",
        "            nodes_pvalueGrand.append(list_nodes[i])\n",
        "    G_prime.add_nodes_from(nodes_pvalue) #affectation des noeuds au graph G_prime \n",
        "    G_prime.add_nodes_from(nodes_pvalueGrand)\n",
        "        #construction de la liste des arretes du graphe G_prime \n",
        "    a=0 \n",
        "    #print(nodes_ww)\n",
        "    for i in range(len(nodes_ww)-1):\n",
        "        a=i+1\n",
        "        while a<len(nodes_ww):\n",
        "            edges_Gprime.append((nodes_ww[i],nodes_ww[a],{'weight': None}))\n",
        "            a=a+1\n",
        "    #print(edges_Gprime)\n",
        "    G_prime.add_edges_from(edges_Gprime)\n",
        "    return  Datasets.init_graph(G_prime, edge_weight=None, default_groups=\"truehit\")"
      ],
      "metadata": {
        "id": "MVpwOcjJd9vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  fonction pour deplacer les fichiers entre les Amine et Mane"
      ],
      "metadata": {
        "id": "dFUYJ80nALCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def deplacer_fichier(fichier, dossier_destination):\n",
        "    try:\n",
        "        # Vérifier si le fichier existe\n",
        "        if os.path.isfile(fichier):\n",
        "            # Vérifier si le dossier de destination existe\n",
        "            if os.path.isdir(dossier_destination):\n",
        "                # Obtenir le nom du fichier\n",
        "                nom_fichier = os.path.basename(fichier)\n",
        "                \n",
        "                # Déplacer le fichier vers le dossier de destination en écrasant si nécessaire\n",
        "                shutil.move(fichier, os.path.join(dossier_destination, nom_fichier))\n",
        "                print(\"Le fichier a été déplacé avec succès.\")\n",
        "            else:\n",
        "                print(\"Le dossier de destination n'existe pas.\")\n",
        "        else:\n",
        "            print(\"Le fichier spécifié n'existe pas.\")\n",
        "    except Exception as e:\n",
        "        print(\"Une erreur s'est produite lors du déplacement du fichier :\", str(e))\n",
        "\n",
        "\n",
        "\n",
        "def create_folder(nom_dossier,num):\n",
        "    new_name=nom_dossier+str(num)\n",
        "    if not os.path.exists(new_name):\n",
        "    # Créer le dossier\n",
        "      os.makedirs(new_name)\n",
        "      print(\"Le dossier a été créé avec succès.\")\n",
        "    else:\n",
        "      print(\"Le dossier existe déjà.\")\n",
        "    return new_name\n",
        "\n",
        "def move_file(fichier, dossier):\n",
        "    # Vérifier si le fichier existe\n",
        "    if os.path.exists(fichier):\n",
        "        # Vérifier si le dossier de destination existe\n",
        "        if os.path.exists(dossier):\n",
        "            # Déplacer le fichier dans le dossier de destination\n",
        "            shutil.move(fichier, dossier)\n",
        "            print(\"Le fichier a été déplacé avec succès dans le dossier.\")\n",
        "        else:\n",
        "            print(\"attention le dosssier n'exite pas .\")\n",
        "    else:\n",
        "        print(\" attention le fichier n'existe pas.\")\n",
        "\n",
        "\n",
        "def deplacer_fichiers(dossier_source, dossier_destination):\n",
        "    # Parcours de tous les fichiers du dossier source\n",
        "    for fichier in os.listdir(dossier_source):\n",
        "        chemin_fichier_source = os.path.join(dossier_source, fichier)\n",
        "        if os.path.isfile(chemin_fichier_source):\n",
        "            chemin_fichier_destination = os.path.join(dossier_destination, fichier)\n",
        "            shutil.move(chemin_fichier_source, chemin_fichier_destination)\n",
        "\n",
        "\n",
        "\n",
        "def transfer_file(dossier_source,dossier_destination):\n",
        "\n",
        "    # Parcours de tous les fichiers du dossier source\n",
        "    for fichier in os.listdir(dossier_source):\n",
        "        chemin_fichier_source = os.path.join(dossier_source, fichier)\n",
        "        if os.path.isfile(chemin_fichier_source):\n",
        "            chemin_fichier_destination = os.path.join(dossier_destination, fichier)\n",
        "            shutil.move(chemin_fichier_source, chemin_fichier_destination)\n",
        "\n",
        "\n",
        "def delete_folder(nom_dossier): \n",
        "    # Vérifier si le dossier existe\n",
        "    if os.path.exists(nom_dossier):\n",
        "        # Supprimer le dossier et son contenu\n",
        "        shutil.rmtree(nom_dossier)\n",
        "        print(\"Le dossier a été supprimé avec succès.\")\n",
        "    else:\n",
        "        print(\"Le dossier n'existe pas.\")"
      ],
      "metadata": {
        "id": "CLiK7N8xeWwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## definition de la classe Mane"
      ],
      "metadata": {
        "id": "3fLmUmvnAbQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Mane(nn.Module):  # ready for cluster, no cache cleaning or loop shortening\n",
        "    def __init__(self, params, len_common_nodes, embed_freq, batch_size, negative_sampling_size=10):\n",
        "        super(Mane, self).__init__()\n",
        "        self.n_embedding = len_common_nodes\n",
        "        self.embed_freq = embed_freq\n",
        "        self.num_net = params.nviews\n",
        "        self.negative_sampling_size = negative_sampling_size\n",
        "        self.node_embeddings = nn.ModuleList()\n",
        "        self.neigh_embeddings = nn.ModuleList()\n",
        "        self.embedding_dim = params.dimensions\n",
        "        self.device = params.device\n",
        "        for n_net in range(self.num_net):  # len(G)\n",
        "            self.node_embeddings.append(nn.Embedding(len_common_nodes, self.embedding_dim))\n",
        "            self.neigh_embeddings.append(nn.Embedding(len_common_nodes, self.embedding_dim))\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, count, shuffle_indices_nets, nodes_idx_nets, neigh_idx_nets, hyp1, hyp2):\n",
        "        cost1 = [nn.functional.logsigmoid(torch.bmm(self.neigh_embeddings[i](Variable(torch.LongTensor(\n",
        "            neigh_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(self.device))).unsqueeze(\n",
        "            2).view(\n",
        "            len(shuffle_indices_nets[i][count:count + self.batch_size]), -1,\n",
        "            self.embedding_dim), self.node_embeddings[i](Variable(\n",
        "            torch.LongTensor(nodes_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(\n",
        "                self.device))).view(\n",
        "            len(shuffle_indices_nets[i][count:count + self.batch_size]), -1).unsqueeze(\n",
        "            2))).squeeze().mean() + nn.functional.logsigmoid(torch.bmm(self.neigh_embeddings[i](\n",
        "            self.embed_freq.multinomial(\n",
        "                len(shuffle_indices_nets[i][count:count + self.batch_size]) * self.neigh_embeddings[i](Variable(\n",
        "                    torch.LongTensor(\n",
        "                        neigh_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(\n",
        "                        self.device))).unsqueeze(\n",
        "                    2).view(len(shuffle_indices_nets[i][count:count + self.batch_size]), -1,\n",
        "                            self.embedding_dim).size(1) * self.negative_sampling_size, replacement=True).to(\n",
        "                self.device)).view(\n",
        "            len(shuffle_indices_nets[i][count:count + self.batch_size]), -1,\n",
        "            self.embedding_dim).neg(), self.node_embeddings[i](Variable(\n",
        "            torch.LongTensor(nodes_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(\n",
        "                self.device))).view(\n",
        "            len(shuffle_indices_nets[i][count:count + self.batch_size]), -1).unsqueeze(2))).squeeze().sum(1).mean(0) for\n",
        "                 i in range(self.num_net)]\n",
        "\n",
        "        # First order collaboration\n",
        "        cost2 = [[hyp1 * (nn.functional.logsigmoid(torch.bmm(self.node_embeddings[j](Variable(torch.LongTensor(\n",
        "            nodes_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(self.device))).unsqueeze(\n",
        "            2).view(\n",
        "            len(shuffle_indices_nets[i][count:count + self.batch_size]), -1, self.embedding_dim),\n",
        "            self.node_embeddings[i](Variable(torch.LongTensor(\n",
        "                nodes_idx_nets[i][shuffle_indices_nets[i][\n",
        "                                  count:count + self.batch_size]]).to(self.device))).view(\n",
        "                len(shuffle_indices_nets[i][\n",
        "                    count:count + self.batch_size]), -1).unsqueeze(\n",
        "                2))).squeeze().mean() + nn.functional.logsigmoid(\n",
        "            torch.bmm(self.node_embeddings[j](self.embed_freq.multinomial(\n",
        "                len(shuffle_indices_nets[i][count:count + self.batch_size]) * self.node_embeddings[j](Variable(\n",
        "                    torch.LongTensor(\n",
        "                        nodes_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(\n",
        "                        self.device))).unsqueeze(\n",
        "                    2).view(len(shuffle_indices_nets[i][count:count + self.batch_size]), -1, self.embedding_dim).size(\n",
        "                    1) * self.negative_sampling_size,\n",
        "                replacement=True).to(self.device)).view(len(shuffle_indices_nets[i][count:count + self.batch_size]), -1,\n",
        "                                                        self.embedding_dim).neg(), self.node_embeddings[i](Variable(\n",
        "                torch.LongTensor(\n",
        "                    nodes_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(self.device))).view(\n",
        "                len(shuffle_indices_nets[i][count:count + self.batch_size]), -1).unsqueeze(2))).squeeze().sum(1).mean(\n",
        "            0))\n",
        "                  for i in range(self.num_net) if i != j] for j in range(self.num_net)]\n",
        "\n",
        "        # Second order collaboration\n",
        "\n",
        "        cost3 = [[hyp2 * (nn.functional.logsigmoid(torch.bmm(self.neigh_embeddings[j](Variable(torch.LongTensor(\n",
        "            neigh_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(self.device))).unsqueeze(\n",
        "            2).view(\n",
        "            len(shuffle_indices_nets[i][count:count + self.batch_size]), -1, self.embedding_dim),\n",
        "            self.node_embeddings[i](Variable(torch.LongTensor(\n",
        "                nodes_idx_nets[i][shuffle_indices_nets[i][\n",
        "                                  count:count + self.batch_size]]).to(self.device))).view(\n",
        "                len(shuffle_indices_nets[i][\n",
        "                    count:count + self.batch_size]), -1).unsqueeze(\n",
        "                2))).squeeze().mean() + nn.functional.logsigmoid(\n",
        "            torch.bmm(self.neigh_embeddings[j](self.embed_freq.multinomial(\n",
        "                len(shuffle_indices_nets[i][count:count + self.batch_size]) * self.neigh_embeddings[j](Variable(\n",
        "                    torch.LongTensor(\n",
        "                        neigh_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(\n",
        "                        self.device))).unsqueeze(\n",
        "                    2).view(len(shuffle_indices_nets[i][count:count + self.batch_size]), -1, self.embedding_dim).size(\n",
        "                    1) * self.negative_sampling_size,\n",
        "                replacement=True).to(self.device)).view(len(shuffle_indices_nets[i][count:count + self.batch_size]), -1,\n",
        "                                                        self.embedding_dim).neg(), self.node_embeddings[i](Variable(\n",
        "                torch.LongTensor(\n",
        "                    nodes_idx_nets[i][shuffle_indices_nets[i][count:count + self.batch_size]]).to(self.device))).view(\n",
        "                len(shuffle_indices_nets[i][count:count + self.batch_size]), -1).unsqueeze(2))).squeeze().sum(1).mean(\n",
        "            0))\n",
        "                  for i in range(self.num_net) if i != j] for j in range(self.num_net)]\n",
        "\n",
        "        sum_cost2 = []\n",
        "        [[sum_cost2.append(j) for j in i] for i in cost2]\n",
        "\n",
        "        sum_cost3 = []\n",
        "        [[sum_cost3.append(j) for j in i] for i in cost3]\n",
        "\n",
        "        return -(torch.mean(torch.stack(cost1)) + sum(sum_cost2) / len(sum_cost2) + sum(sum_cost3) / len(sum_cost3)) / 3\n",
        "\n",
        "\n",
        "\n",
        "def read_graphs(current_path, n_views):\n",
        "    \"\"\"\n",
        "        Read graph/network data for each view from an adjlist (from networkx package)\n",
        "\n",
        "    :param current_path: path for graph data\n",
        "    :param n_views: number of views\n",
        "    :return: A list of graphs\n",
        "    \"\"\"\n",
        "    entries = os.listdir(current_path)\n",
        "    G = []\n",
        "    if len(entries) != n_views:\n",
        "        print(\"WARNING: Number of networks in the folder is not equal to number of views setting.\")\n",
        "    for n_net in range(n_views):\n",
        "        G.append(nx.read_adjlist(current_path + entries[n_net]))\n",
        "        print(\"Network \", (n_net + 1), \": \", entries[n_net])\n",
        "    return G\n",
        "\n",
        "\n",
        "def read_word2vec_pairs(current_path, nviews):\n",
        "    \"\"\"\n",
        "\n",
        "    :param current_path: path for two files, one keeps only the node indices, the other keeps only the neighbor node\n",
        "    indices of already generated pairs (node,neighbor), i.e, node indices and neighbor indices are kept separately.\n",
        "    method \"construct_word2vec_pairs\" can be used to obtain these files.\n",
        "    :E.g.:\n",
        "\n",
        "      for pairs (9,2) (4,5) (8,6) one file keeps 9 4 8 the other file keeps 2 5 6.\n",
        "\n",
        "    :param nviews: number of views\n",
        "    :return: Two lists for all views, each list keeps the node indices of node pairs (node, neigh).\n",
        "    nodes_idx_nets for node, neigh_idx_nets for neighbor\n",
        "    \"\"\"\n",
        "\n",
        "    nodes_idx_nets = []\n",
        "    neigh_idx_nets = []\n",
        "\n",
        "    for n_net in range(nviews):\n",
        "        neigh_idx_nets.append(np.loadtxt(current_path + \"/neighidxPairs_\" + str(n_net + 1) + \".txt\"))\n",
        "        nodes_idx_nets.append(np.loadtxt(current_path + \"/nodesidxPairs_\" + str(n_net + 1) + \".txt\"))\n",
        "    return nodes_idx_nets, neigh_idx_nets\n",
        "\n",
        "\n",
        "def degree_nodes_common_nodes(G, common_nodes, node2idx):\n",
        "    \"\"\"\n",
        "    Assigns scores for negative sampling distribution\n",
        "    \"\"\"\n",
        "    degrees_idx = dict((node2idx[v], 0) for v in common_nodes)\n",
        "    multinomial_nodesidx = []\n",
        "    for node in common_nodes:\n",
        "        degrees_idx[node2idx[node]] = sum([G[n].degree(node) for n in range(len(G))])\n",
        "    for node in common_nodes:\n",
        "        multinomial_nodesidx.append(degrees_idx[node2idx[node]] ** (0.75))\n",
        "\n",
        "    return multinomial_nodesidx\n",
        "\n",
        "\n",
        "def mainMane():\n",
        "    \"\"\"\n",
        "    Initialize parameters and train\n",
        "    \"\"\"\n",
        "\n",
        "    params = get_parser()\n",
        "    print(params)\n",
        "\n",
        "    if torch.cuda.is_available() and not params.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, you may try cuda with --cuda\")\n",
        "    device = 'cuda:0' if torch.cuda.is_available() and params.cuda else 'cpu'\n",
        "    params.device = device\n",
        "    print(\"Running on device: \", device)\n",
        "    G = read_graphs(params.input_graphs + params.dataset, params.nviews)\n",
        "    common_nodes = sorted(set(G[0]).intersection(*G))\n",
        "    print('Number of common/core nodes in all networks: ', len(common_nodes))\n",
        "    node2idx = {n: idx for (idx, n) in enumerate(common_nodes)}\n",
        "    print(\"node2idx\",node2idx)\n",
        "    idx2node = {idx: n for (idx, n) in enumerate(common_nodes)}\n",
        "\n",
        "    if params.read_pair:\n",
        "\n",
        "        nodes_idx_nets, neigh_idx_nets = read_word2vec_pairs(\"MANE/\"+params.input_pairs + params.dataset, params.nviews)\n",
        "\n",
        "    else:\n",
        "        nodes_idx_nets = []\n",
        "        neigh_idx_nets = []\n",
        "        for n_net in range(params.nviews):\n",
        "            view_id = n_net + 1\n",
        "            print(\"View \", view_id)\n",
        "\n",
        "            nodes_idx, neigh_idx = generate_pairs.construct_word2vec_pairs(G[n_net], view_id, common_nodes, params.p,\n",
        "                                                                           params.q, params.window_size,\n",
        "                                                                           params.num_walks,\n",
        "                                                                           params.walk_length,\n",
        "                                                                           params.output_pairs , #+ params.dataset\n",
        "                                                                           node2idx)\n",
        "\n",
        "            nodes_idx_nets.append(nodes_idx)\n",
        "            neigh_idx_nets.append(neigh_idx)\n",
        "\n",
        "    multinomial_nodes_idx = degree_nodes_common_nodes(G, common_nodes, node2idx)\n",
        "\n",
        "    embed_freq = Variable(torch.Tensor(multinomial_nodes_idx))\n",
        "\n",
        "    model = Mane(params, len(common_nodes), embed_freq, params.batch_size)\n",
        "    model.to(device)\n",
        "\n",
        "    epo = 0\n",
        "    min_pair_length = nodes_idx_nets[0].size\n",
        "    for n_net in range(params.nviews):\n",
        "        if min_pair_length > nodes_idx_nets[n_net].size:\n",
        "            min_pair_length = nodes_idx_nets[n_net].size\n",
        "    print(\"Total number of pairs: \", min_pair_length)\n",
        "    print(\"Training started! \\n\")\n",
        "\n",
        "    while epo <= params.epochs - 1:\n",
        "        start_init = time.time()\n",
        "\n",
        "        epo += 1\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=params.learning_rate)\n",
        "        running_loss = 0\n",
        "        num_batches = 0\n",
        "        shuffle_indices_nets = []\n",
        "        fifty = False\n",
        "\n",
        "        for n_net in range(params.nviews):\n",
        "            shuffle_indices = [x for x in range(nodes_idx_nets[n_net].size)]\n",
        "            random.shuffle(shuffle_indices)\n",
        "            shuffle_indices_nets.append(shuffle_indices)\n",
        "        for count in range(0, min_pair_length, params.batch_size):\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(count, shuffle_indices_nets, nodes_idx_nets, neigh_idx_nets, params.alpha, params.beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.detach().item()\n",
        "            num_batches += 1\n",
        "            if int(num_batches % 100) == 0:\n",
        "                print(num_batches, \" batches completed\\n\")\n",
        "            elif not fifty and (count / min_pair_length) * 100 > 50:\n",
        "                print(\"############# 50% epoch is completed #################\\n\")\n",
        "                fifty = True\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        total_loss = running_loss / (num_batches)\n",
        "        elapsed = time.time() - start_init\n",
        "        print('epoch=', epo, '\\t time=', elapsed, ' seconds\\t total_loss=', total_loss)\n",
        "\n",
        "    concat_tensors = model.node_embeddings[0].weight.detach().cpu()\n",
        "    print('Embedding of view ', 1, ' ', concat_tensors)\n",
        "\n",
        "    for i_tensor in range(1, model.num_net):\n",
        "        print('Embedding of view ', (i_tensor + 1), ' ', model.node_embeddings[i_tensor].weight.detach().cpu())\n",
        "        concat_tensors = torch.cat((concat_tensors, model.node_embeddings[i_tensor].weight.detach().cpu()), 1)\n",
        "\n",
        "    emb_file = params.output + params.dataset + \"Embedding_\" + \"concatenated_without_attention\" + '_epoch_' + str(\n",
        "        epo) + \"_\" + \".txt\"\n",
        "    embed_result = np.array(concat_tensors)\n",
        "    fo = open(emb_file, 'a+')\n",
        "    for idx in range(len(embed_result)):\n",
        "        word = (idx2node[idx])\n",
        "        fo.write(word + ' ' + ' '.join(\n",
        "            map(str, embed_result[idx])) + '\\n')\n",
        "    fo.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dv91_9aXe3HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### affichage des graphe des courbe d'apprentissage de former l'embbeding "
      ],
      "metadata": {
        "id": "RRkE8D_ZBRIs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwQcrLlkBQlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## definition d'Amine "
      ],
      "metadata": {
        "id": "ruqafGtSAYmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "arg=argumentAmine()\n",
        "destinationPair =\"MANE/output/pairs\"\n",
        "destinationGrap =\"MANE/output/pairs\"\n",
        "dossierdetraitement=\"MANE/data/networks/Artificiel_Data\"\n",
        "sourcePair=\"output/pairs\"\n",
        "__name__= \"main\"\n",
        "if __name__ == \"main\":\n",
        "    # Entry point.\n",
        "    #arg = parse_arguments()\n",
        "\n",
        "    # Use Node2vec model\n",
        "    model = models.Node2vec()\n",
        "\n",
        "    # Use aggregation zscore as fitness function.\n",
        "    fitness_fun = lambda the_graph, clus: Scores.aggregation_from_pvalue(\n",
        "        the_graph, clus, \"weight\"\n",
        "    )\n",
        "\n",
        "    # set characteristic of dense network\n",
        "    P_PROB = 0.09\n",
        "    Q_PROB = 0.70\n",
        "    NB_INITIAL_NODES = 3\n",
        "\n",
        "    f1_scores = []\n",
        "    if arg.outfile:\n",
        "        outfile = open(arg.outfile, \"w\")\n",
        "        outfile.write(\"#graph,time(s),nb found,real size,true hits,pvalue\\n\")\n",
        "    for ctr in range(arg.number_of_runs):\n",
        "        if arg.graph_generation == \"guyondata\":\n",
        "            G = Datasets.get_guyon_graph(ctr + 1)\n",
        "        elif arg.graph_generation == \"gencat\":\n",
        "            G = Datasets.get_gencat_graph(\n",
        "                arg.network_size,\n",
        "                arg.network_size * 16,\n",
        "                arg.nb_modules,\n",
        "                arg.nb_modules * [arg.module_size],\n",
        "                ctr,\n",
        "            )\n",
        "        else:\n",
        "            G = Datasets.get_scale_free_graph(\n",
        "                arg.network_size,\n",
        "                NB_INITIAL_NODES,\n",
        "                arg.nb_modules,\n",
        "                arg.module_size,\n",
        "                P_PROB,\n",
        "                Q_PROB,\n",
        "                ctr,\n",
        "            )\n",
        "            if arg.removed_edges > 0:\n",
        "                nbtoremove = int(G.number_of_edges() * arg.removed_edges)\n",
        "                rng = Random(ctr)\n",
        "                edges_to_remove = rng.sample(G.edges, nbtoremove)\n",
        "                G.remove_edges_from(edges_to_remove)\n",
        "                G.graph[\"nb_edges\"] = G.number_of_edges()\n",
        "\n",
        "        \n",
        "        # contrution du graphe seecondaire \n",
        "        G_p=graph_artificiel2(G)  # appel de la fonction pour contruire le graphe \n",
        "        # nommage de fichier pour stocke les graphes  ** numeroRun+sub1.txt\n",
        "        graphe_vue1=str(ctr+1)+\"_sub1.txt\" \n",
        "        graphe_vue2=str(ctr+1)+\"_sub2.txt\" # nommage de fichier pour stocke les graphes  ** numeroRun+sub2.txt\n",
        "        #ecriture du graphe dans un fichier \n",
        "        nx.write_adjlist( G, graphe_vue1) # sauvegarde du graphe dans un fichier \n",
        "        nx.write_adjlist( G_p, graphe_vue2)\n",
        "        # deplacement dans le dossier data de Mane\n",
        "        move_file(graphe_vue1,dossierdetraitement) \n",
        "        move_file(graphe_vue2,dossierdetraitement) \n",
        "        # appel de la fonction Mane \n",
        "        mainMane()\n",
        "        # creer le dossier de stokage de des pairs creer \n",
        "        destPair=create_folder(destinationPair,str(ctr+1))\n",
        "        destGrap=create_folder(destinationGrap,str(ctr+1))\n",
        "        # deplacer les fichier du dossier output/pair dans un autre dossier de stockage pour un calcul ulterieur \n",
        "        deplacer_fichiers(sourcePair,destPair)\n",
        "        #deplacer les fichiers des datatratemengt  traité vers un autre dossier \n",
        "        deplacer_fichiers(dossierdetraitement,destGrap)\n",
        "        # recuperation de du fichier embedding \n",
        "        \n",
        "        \n",
        "        # initialize the model\n",
        "        #model.init(G)\n",
        "\n",
        "        # call module detection method\n",
        "        start_time = time.perf_counter()\n",
        "        module_detection = ModuleDetection(\n",
        "            G, model, fitness_fun, background_correction=True\n",
        "        )\n",
        "\n",
        "        # do the prediction\n",
        "        results = module_detection.predict_modules(\n",
        "            max_nb_modules=arg.nb_modules, cutoff=0.05\n",
        "        )\n",
        "        end_time = time.perf_counter()\n",
        "        pvalue = 0\n",
        "        if results:\n",
        "            pvalue = results[0][2]\n",
        "        truehits = Datasets.get_groups(G)\n",
        "        print(\"truehits\", truehits)\n",
        "\n",
        "        pred = set()\n",
        "        nb_clust = 0\n",
        "        for cl in results:\n",
        "            if arg.target_module_size:\n",
        "                if not pred:\n",
        "                    pred.update(cl[0])\n",
        "                    continue\n",
        "                if len(pred) < arg.target_module_size:\n",
        "                    if abs(arg.target_module_size - len(pred)) > abs(\n",
        "                        arg.target_module_size - len(pred) - len(cl[0])\n",
        "                    ):\n",
        "                        pred.update(cl[0])\n",
        "                        continue\n",
        "                    else:\n",
        "                        break\n",
        "                else:\n",
        "                    break\n",
        "            else:\n",
        "                nb_clust += 1\n",
        "                if nb_clust > arg.nb_modules:\n",
        "                    break\n",
        "                pred.update(cl[0])\n",
        "\n",
        "        # merge the content of the true hits in a set\n",
        "        th = set()\n",
        "        for key, value in truehits.items():\n",
        "            th.update(value)\n",
        "\n",
        "        nb_pred = len(pred)\n",
        "        nb_th = len(th)\n",
        "        nb_found = len(pred & th)\n",
        "\n",
        "        if arg.outfile:\n",
        "            perf = end_time - start_time\n",
        "            outfile.write(\n",
        "                f\"{ctr + 1},{perf:.2f},{nb_pred},{nb_th},{nb_found},{pvalue}\\n\"\n",
        "            )\n",
        "            outfile.flush()\n",
        "        if not arg.verbose:\n",
        "            continue\n",
        "        # calculate the F1 score\n",
        "        res = []\n",
        "        f1_score = Scores.measure_f1(G, th, pred)\n",
        "        res.append((len(pred), len(pred & th), f1_score))\n",
        "\n",
        "        f1_scores.append(f1_score)\n",
        "\n",
        "        # compute the stats\n",
        "        quartiles = np.percentile(f1_scores, [25, 50, 75])\n",
        "        minim = min(f1_scores)\n",
        "        maxim = max(f1_scores)\n",
        "        iqr = quartiles[2] - quartiles[0]\n",
        "        high = min(maxim, quartiles[2] + iqr * 1.5)\n",
        "        low = max(minim, quartiles[0] - iqr * 1.5)\n",
        "        out_high = [str(f\"{x:.5f}\") for x in f1_scores if x > high]\n",
        "        out_low = [str(f\"{x:.5f}\") for x in f1_scores if x < low]\n",
        "        print(\n",
        "            f\"{ctr + 1} len={nb_pred} true_hits={nb_th} found={nb_found} f1_score={f1_scores[-1]:.5f}, mean={statistics.mean(f1_scores):.5f}, variance={statistics.pstdev(f1_scores):.5f}, Q25={quartiles[0]:.5f}, Q50={quartiles[1]:.5f}, Q75={quartiles[2]:.5f}, low={low:.5f}, high={high:.5f}, out_low=[{','.join(out_low)}], out_high=[{','.join(out_high)}] pvalue={pvalue}\"\n",
        "        )\n",
        "    if arg.outfile:\n",
        "        outfile.close()\n"
      ],
      "metadata": {
        "id": "Ej_AapTnfJqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfeVDKcCBmrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rechercher de la similarité entre les noeuds "
      ],
      "metadata": {
        "id": "stfzYn64AwrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ajouter_element(ensemble, element):\n",
        "    ensemble.add(element)\n",
        "\n",
        "fichier = \"MANE/output/emb/Artificiel_Data/Embedding_concatenated_without_attention_epoch_1000_.txt\"\n",
        "vectors = np.loadtxt(fichier)\n",
        "labels = vectors[:, 0]  # Extraction de la première colonne (labels)\n",
        "vectors = vectors[:, 1:]  # Extraction des autres colonnes (composantes vectorielles)\n",
        "for id_noeud in range(1000):\n",
        "    reference_vector = vectors[id_noeud ]\n",
        "    # Calcul de la similarité cosinus\n",
        "    similarities = []\n",
        "    for vector in vectors:\n",
        "        similarity = 1 - distance.cosine(reference_vector, vector)\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    # Sélection des indices des vecteurs avec une similarité supérieure à 0.95\n",
        "    \n",
        "    # Sélection des indices des vecteurs avec une similarité supérieure à 0.95\n",
        "    selected_indices = [i for i, similarity in enumerate(similarities) if similarity > 0.65]\n",
        "    selected_indices = selected_indices[:10]  # Sélection des 10 premiers indices\n",
        "\n",
        "    # Sélection des labels et des vecteurs correspondants\n",
        "    selected_labels = labels[selected_indices]\n",
        "    selected_vectors = vectors[selected_indices]\n",
        "    mon_ensemble = []\n",
        "    # Affichage des vecteurs similaires avec leurs labels*\n",
        "    \n",
        "    for label, vector in zip(selected_labels, selected_vectors):\n",
        "       # print(\"Label:\", str(int(label)))\n",
        "       mon_ensemble.append(int(label))\n",
        "        #ajouter_element(mon_ensemble, label)\n",
        "    if len(mon_ensemble)>2 :\n",
        "        print(\"######################################\")\n",
        "        print(mon_ensemble)  # Affiche : {1, 2, 3, 4}\n",
        "\n",
        "        #print(\"Vecteur:\", vector)"
      ],
      "metadata": {
        "id": "aiog83v-faFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KEoVdXSjBGxy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhJsWoZSBFkH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}